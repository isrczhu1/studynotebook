# 强化学习

​																							*2020-11-25*

强化学习是和监督学习和非监督学习并列的第三种机器学习方法，								



![img](https://images2018.cnblogs.com/blog/1042406/201807/1042406-20180729161444252-1175243635.png)

第一个问题是预测，即给定强化学习的6个要素：状态集SS, 动作集AA, 模型状态转化概率矩阵PP, 即时奖励RR，衰减因子γγ, 给定策略ππ， 求解该策略的状态价值函数v(π)v(π)

　　　　第二个问题是控制，也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集SS, 动作集AA, 模型状态转化概率矩阵PP, 即时奖励RR，衰减因子γγ, 求解最优的状态价值函数v∗v∗和最优策略π∗

**DQN属于控制**,没有策略。

**状态价值函数**是节点状态的价值，



![image-20201125143909076](C:\Users\ZHZ\AppData\Roaming\Typora\typora-user-images\image-20201125143909076.png)

动作价值函数是状态下执行某个动作的价值。





**蒙特卡罗法**很灵活，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。

**时序差分(Temporal-Difference, TD)**



**经验池中存储S，A，R,  S', END;**

**使用S作为模型输入，得到预测的所有动作的Q值。根据动作A拿到对应Q预测值。**

**使用R 和S'还有AND，计算Q目标值。如果END为true。目标值就为R。如果END为false，就将S'作为模型的输入，得到该状态下所有动作的Q值，选择做大的乘以折扣率。再加上R就是最终目标值。**

**损失函数使用（目标值-预测值）再更新模型参数。**



![image-20201126193743757](C:\Users\ZHZ\AppData\Roaming\Typora\typora-user-images\image-20201126193743757.png)

##### cartPole:



![image-20201126204920801](C:\Users\ZHZ\AppData\Roaming\Typora\typora-user-images\image-20201126204920801.png)

![image-20201126205427748](C:\Users\ZHZ\AppData\Roaming\Typora\typora-user-images\image-20201126205427748.png)

![image-20201126205545071](C:\Users\ZHZ\AppData\Roaming\Typora\typora-user-images\image-20201126205545071.png)







​            out_action = tf.**reduce_sum**(tf.**multiply**(self.stopScore, self.action), reduction_indices=1) 

1. 
   \#两个矩阵相乘

2. x=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])  

3. y=tf.constant([[0,0,1.0],[0,0,1.0],[0,0,1.0]])

4. \#注意这里这里x,y要有相同的数据类型，不然就会因为数据类型不匹配而出错

5. z=tf.**multiply**(x,y)

   #【0，0，3】

 reduction_indices=1  # 按照维度1相加，二维的话就是按行相加。





### **一、tf.reduce_mean** 函数

用于计算张量tensor沿着指定的数轴（tensor的某一维度）上的的平均值，主要用作降维或者计算tensor（图像）的平均值，默认对所有的元素求平均。

- 第一个参数input_tensor： 输入的待降维的tensor;
- 第二个参数axis： 指定的轴，如果不指定，则计算所有元素的均值;
- 第三个参数keep_dims：是否降维度，设置为True，输出的结果保持输入tensor的形状，设置为False，输出结果会降低维度;
- 第四个参数name： 操作的名称;
- 第五个参数 reduction_indices：在以前版本中用来指定轴，已弃用;